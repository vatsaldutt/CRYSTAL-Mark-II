{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'newspaper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnewspaper\u001b[39;00m \u001b[39mimport\u001b[39;00m Article\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscrape_article\u001b[39m(url):\n\u001b[1;32m      4\u001b[0m     article \u001b[39m=\u001b[39m Article(url)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'newspaper'"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "def scrape_article(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text\n",
    "\n",
    "main_text = scrape_article('https://blog.tensorflow.org/2019/05/transformer-chatbot-tutorial-with-tensorflow-2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(main_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conversation, pipeline\n",
    "\n",
    "# Define the conversational AI pipeline\n",
    "chatbot = pipeline(\"conversational\")\n",
    "\n",
    "# Start the conversation\n",
    "print(\"Chatbot: Hello! How can I assist you today?\")\n",
    "conversation = Conversation()\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    # Append user input to the conversation\n",
    "    conversation.add_user_input(user_input)\n",
    "\n",
    "    # Generate response\n",
    "    response = chatbot(conversation)\n",
    "\n",
    "    # Get the generated response\n",
    "    bot_response = response.generated_responses[-1]\n",
    "\n",
    "    # Print bot's response\n",
    "    print(\"Chatbot:\", bot_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vatsal/Desktop/CRYSTAL MARK II/CRYSTAL-venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[1;32m     25\u001b[0m conversation_history\u001b[39m.\u001b[39mappend(user_input)\n\u001b[0;32m---> 26\u001b[0m bot_response \u001b[39m=\u001b[39m generate_response(user_input)\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCrystal:\u001b[39m\u001b[39m\"\u001b[39m, bot_response)\n\u001b[1;32m     28\u001b[0m conversation_history\u001b[39m.\u001b[39mappend(bot_response)\n",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_response\u001b[39m(prompt):\n\u001b[1;32m     11\u001b[0m     \u001b[39m# Encode the input text\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode(conversation_history, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     13\u001b[0m     \u001b[39m# input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt')\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39m# Generate the bot's response\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(input_ids\u001b[39m=\u001b[39minput_ids, max_length\u001b[39m=\u001b[39m\u001b[39m2000\u001b[39m, pad_token_id\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39meos_token_id)\n",
      "File \u001b[0;32m~/Desktop/CRYSTAL MARK II/CRYSTAL-venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2309\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2272\u001b[0m \u001b[39m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2273\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2274\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2292\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2293\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[1;32m   2294\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2295\u001b[0m \u001b[39m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2296\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2307\u001b[0m \u001b[39m            method).\u001b[39;00m\n\u001b[1;32m   2308\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2309\u001b[0m     encoded_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   2310\u001b[0m         text,\n\u001b[1;32m   2311\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2312\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2313\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2314\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2315\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2316\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2317\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2318\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2319\u001b[0m     )\n\u001b[1;32m   2321\u001b[0m     \u001b[39mreturn\u001b[39;00m encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/CRYSTAL MARK II/CRYSTAL-venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2708\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2687\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2688\u001b[0m \u001b[39mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[1;32m   2689\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2704\u001b[0m \u001b[39m        method).\u001b[39;00m\n\u001b[1;32m   2705\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2707\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> 2708\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_padding_truncation_strategies(\n\u001b[1;32m   2709\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2710\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2711\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2712\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2713\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2714\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2715\u001b[0m )\n\u001b[1;32m   2717\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[1;32m   2718\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2719\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2735\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2736\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/CRYSTAL MARK II/CRYSTAL-venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2443\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2441\u001b[0m \u001b[39m# Test if we have a padding token\u001b[39;00m\n\u001b[1;32m   2442\u001b[0m \u001b[39mif\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token_id \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m-> 2443\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2444\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2446\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpad_token\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m})`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2447\u001b[0m     )\n\u001b[1;32m   2449\u001b[0m \u001b[39m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[1;32m   2450\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2451\u001b[0m     truncation_strategy \u001b[39m!=\u001b[39m TruncationStrategy\u001b[39m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[1;32m   2452\u001b[0m     \u001b[39mand\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2455\u001b[0m     \u001b[39mand\u001b[39;00m (max_length \u001b[39m%\u001b[39m pad_to_multiple_of \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m   2456\u001b[0m ):\n",
      "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "# Define a function to generate the bot's response\n",
    "def generate_response(prompt):\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt')\n",
    "    # Generate the bot's response\n",
    "    output = model.generate(input_ids=input_ids, max_length=2000, pad_token_id=tokenizer.eos_token_id)\n",
    "    # Decode the response and return it as a string\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Start the chat loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    print(\"-\"*50)\n",
    "    bot_response = generate_response(user_input)\n",
    "    print(\"Crystal:\", bot_response)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
